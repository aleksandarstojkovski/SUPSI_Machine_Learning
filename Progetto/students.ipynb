{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto Machine Learning\n",
    "Progetto per il corso di Machine Learning - SUPSI DTI 2020/2021.\n",
    "\n",
    "Gruppo formato da:\n",
    "* De Santi Massimo\n",
    "* Aleskandar Stojkovski\n",
    "\n",
    "## Dataset\n",
    "In Portogallo la percentuale di studenti che abbandonano la scuola tra i 18 e i 24 anni è particolarmente alta (40%) rispetto al resto d'europa (15%).\n",
    "L'obiettivo è quello di poter prevedere possibili fallimenti in modo da offrire supporto in maniera tempestiva agli studenti in difficoltà. Inoltre, potrebbe essere interessante capire se esistono fattori esterni che influenzano il rendimento degli studenti.\n",
    "\n",
    "\n",
    "\n",
    "Il seguente dataset contiene i dati di due scuole superiori Portoghesi: \n",
    "\n",
    "| Scuola | Osservazioni |\n",
    "| :-- | --- | \n",
    "| Gabriel Pereira (GP) | 772 |\n",
    "| Mousinho da Silveira (MS) | 272 |\n",
    "\n",
    "Attraverso due distinti **dataset** vengono analizzate le performance di due **materie** nell'arco del triennio di studi:\n",
    "\n",
    "| Dataset | Materia | Osservazioni |\n",
    "| :-- | :-- | --- | \n",
    "| *student-mat.csv* | Matematica | 395 |\n",
    "| *student-por.csv* | Lingua Portoghese | 649 |\n",
    "\n",
    "Tra le variabili **indipendenti** (attributi) troviamo:\n",
    "- attributi demografici\n",
    "- attributi sociali\n",
    "- attributi relativi alla scuola\n",
    "\n",
    "Variabili **dipendenti** (valore target):\n",
    "- voti dei rispettivi anni (`G1`, `G2`, `G3`) su una scala \\[0..20\\]\n",
    "\n",
    "\n",
    "## Descrizione Attributi\n",
    "\n",
    "### Attributi Numerici\n",
    "| i | col | description |\n",
    "| --- | :- | :- |\n",
    "| 3  | age        | student's age (numeric: from 15 to 22)\n",
    "| 15 | failures   | number of past class failures (numeric: n if 1<=n<3, else 4)\n",
    "| 30 | absences   | number of school absences (numeric: from 0 to 93)\n",
    "\n",
    "### Attributi Semi-Numerici\n",
    "| i | col | description |\n",
    "| --- | :- | :- |\n",
    "| 7  | Medu       | mother's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "| 8  | Fedu       | father's education (numeric: 0 - none,  1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)\n",
    "| 13 | traveltime | home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n",
    "| 14 | studytime  | weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n",
    "| 24 | famrel     | quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "| 25 | freetime   | free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "| 26 | goout      | going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "| 27 | Dalc       | workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "| 28 | Walc       | weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n",
    "| 29 | health     | current health status (numeric: from 1 - very bad to 5 - very good)\n",
    "\n",
    "### Attributi Categorici\n",
    "\n",
    "| i | col | description |\n",
    "| --- | :- | :- |\n",
    "| 1  | school     | student's school (binary: \"GP\" - Gabriel Pereira or \"MS\" - Mousinho da Silveira)\n",
    "| 2  | sex        | student's sex (binary: \"F\" - female or \"M\" - male)\n",
    "| 4  | address    | student's home address type (binary: \"U\" - urban or \"R\" - rural)\n",
    "| 5  | famsize    | family size (binary: \"LE3\" - less or equal to 3 or \"GT3\" - greater than 3)\n",
    "| 6  | Pstatus    | parent's cohabitation status (binary: \"T\" - living together or \"A\" - apart)\n",
    "| 9  | Mjob       | mother's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "| 10 | Fjob       | father's job (nominal: \"teacher\", \"health\" care related, civil \"services\" (e.g. administrative or police), \"at_home\" or \"other\")\n",
    "| 11 | reason     | reason to choose this school (nominal: close to \"home\", school \"reputation\", \"course\" preference or \"other\")\n",
    "| 12 | guardian   | student's guardian (nominal: \"mother\", \"father\" or \"other\")\n",
    "| 16 | schoolsup  | extra educational support (binary: yes or no)\n",
    "| 17 | famsup     | family educational support (binary: yes or no)\n",
    "| 18 | paid       | extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "| 19 | activities | extra-curricular activities (binary: yes or no)\n",
    "| 20 | nursery    | attended nursery school (binary: yes or no)\n",
    "| 21 | higher     | wants to take higher education (binary: yes or no)\n",
    "| 22 | internet   | Internet access at home (binary: yes or no)\n",
    "| 23 | romantic   | with a romantic relationship (binary: yes or no)\n",
    "\n",
    "### Attributi Target\n",
    "| i | col | description |\n",
    "| --- | :- | :- |\n",
    "| 31 | G1 | first period grade (numeric: from 0 to 20) |\n",
    "| 31 | G2 | second period grade (numeric: from 0 to 20) |\n",
    "| 32 | G3 | final grade (numeric: from 0 to 20, output target) |\n",
    "\n",
    "\n",
    "## Note\n",
    "* L'attributo target `G3` (Voto terzo e ultimo anno) ha una forte correlazione con gli attributi `G1` (voto primo anno) e `G2` (voto secondo anno). E' piu' difficile predire `G3` senza `G1` e `G2`, ma questa predizione e' anche piu' utile.\n",
    "* Eseguendo un `group by` per attributi demografici e' possibile identificare gli studenti univoci come mostrato nel file student-merge.R\n",
    "\n",
    "## Riferimenti\n",
    "\n",
    "* Link dataset: https://archive.ics.uci.edu/ml/datasets/Student+Performance\n",
    "* P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7. http://www3.dsi.uminho.pt/pcortez/student.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.markers as mrk\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, ParameterGrid, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# this allows plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset urls\n",
    "base_url = \"https://raw.githubusercontent.com/aleksandarstojkovski/SUPSI_Machine_Learning/main/dataset\"\n",
    "portuguese_dataset_url = f\"{base_url}/student-por.csv\"\n",
    "math_dataset_url = f\"{base_url}/student-mat.csv\"\n",
    "\n",
    "# dataframes\n",
    "df_por = pd.read_csv(portuguese_dataset_url, sep=';')\n",
    "df_math = pd.read_csv(math_dataset_url, sep=';')\n",
    "df = pd.concat([df_por, df_math], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center all the matplotlib graphs in Jupyter notebook\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esplorazione dei dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usiamo la classe **DataFrame** per ottenere informazioni di sintesi sui dati caricati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target columns\n",
    "target_cols = ['G1','G2','G3']\n",
    "df_target = df[target_cols]\n",
    "print(f'target columns: {len(df_target.columns)}')\n",
    "df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric columns\n",
    "df_notarget = df.drop(target_cols, axis=1)\n",
    "df_num = df_notarget._get_numeric_data()\n",
    "print(f'numeric columns: {len(df_num.columns)}')\n",
    "df_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical columns \n",
    "df_cat = df_notarget.drop(df_num.columns, axis=1)\n",
    "print(f'categorical columns: {len(df_cat.columns)}')\n",
    "df_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e' possibile estrarre il numero di righe e di colonne tramite la proprieta' shape del DataFrame\n",
    "num_rows, num_cols = df.shape\n",
    "print(f'Numero righe: {num_rows}')\n",
    "print(f'Numero colonne: {num_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa un sommario delle features (colonne) del DataFrame: indice, nome, # valori nulli, tipo\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controllo se ci sono righe duplicate (0=falso)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controllo numero di valori univoci\n",
    "# df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stampa un sommario statistico: dispersione e forma della distribuzione del DataFrame \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizzazione grafica della distribuzione dei valori delle features (istogrammi)\n",
    "df.hist(bins=50, figsize=(20,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studenti per scuola\n",
    "by_school = df['school'].value_counts().reset_index()\n",
    "by_school.columns = ['school', 'students_count']\n",
    "by_school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificare gli studenti univoci\n",
    "Anche se sono coinvolte 2 differenti scuole, le **1044** osservazioni non rappresentano studenti univoci.  \n",
    "E' ragionavole pensare che alcuni studenti seguano sia il corso di Matematica che quello di Lingua Portoghese.  \n",
    "Pur non avendo accesso ad un ID univoco per studente, possiamo cercare di raggruppare i dati per caratteristiche demografiche.  \n",
    "Dai risultati ottenuti notiamo che ci sono **672** studenti univoci, divisi in **300** studenti che seguono una materia sola e altri **372** che seguono sia Matematica che Potroghese. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_por) #649 rows\n",
    "len(df_math) #395 rows\n",
    "len(df) #1044 total rows\n",
    "\n",
    "#group_cols = [\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\",\"nursery\",\"internet\"]\n",
    "group_cols = [\"school\",\"sex\",\"age\",\"address\",\"famsize\",\"Pstatus\",\"Medu\",\"Fedu\",\"Mjob\",\"Fjob\",\"reason\", \"nursery\",\"internet\",\n",
    "              \"guardian\", \"traveltime\",\"famrel\",\"freetime\",\"goout\",\"Dalc\",\"Walc\"]\n",
    "size = df.groupby(group_cols).size().reset_index() \n",
    "len(size[size[0] > 2]) #0 studenti, OK avendo 2 materie non ci aspettiamo piu' di 2 sovrapposizioni\n",
    "print(f'Numero studenti totali: {len(size[size[0] >= 1])}') #672 \n",
    "print(f'Numero studenti che seguono sia Matematica che Portoghese: {len(size[size[0] == 2])}') #372 \n",
    "print(f'Numero studenti che seguono solo una materia: {len(size[size[0] == 1])}') #300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi caratteristiche demografiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_count = df['sex'].value_counts().reset_index()\n",
    "gender_count.columns = ['gender', 'count']\n",
    "gender_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_function(val):\n",
    "    return f'{val / 100 * len(df):.0f}\\n{val:.0f}%'\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 5))\n",
    "df.groupby('sex').size().plot(kind='pie', autopct=label_function, ax=ax1, colors=['pink', 'royalblue'])\n",
    "df.groupby('school').size().plot(kind='pie', autopct=label_function, ax=ax2)\n",
    "ax1.set_xlabel('Maschi/Femmine', size=15)\n",
    "ax2.set_xlabel('Scuola', size=15)\n",
    "ax1.xaxis.set_label_position('top') \n",
    "ax2.xaxis.set_label_position('top') \n",
    "ax1.set_ylabel('')\n",
    "ax2.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "age_sex_total = df.groupby(['age','sex'])['G3'].count().reset_index()\n",
    "age_sex_pass = df.loc[df['G3']>=10].groupby(['age','sex'])['G3'].count().reset_index()\n",
    "age_sex_fail = df.loc[df['G3']<10].groupby(['age','sex'])['G3'].count().reset_index()\n",
    "\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=1,\n",
    "    shared_xaxes=True\n",
    ")    \n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Promossi', \n",
    "           x=list(age_sex_pass.query('sex == \"M\"')['age']), \n",
    "           y=list(age_sex_pass.query('sex == \"M\"')['G3']), \n",
    "           text=list(age_sex_pass.query('sex == \"M\"')['G3']),\n",
    "           marker_color = ['green'] * 10,\n",
    "           showlegend=False\n",
    "          ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.append_trace(\n",
    "    go.Bar(name='Bocciati', \n",
    "           x=list(age_sex_fail.query('sex == \"M\"')['age']), \n",
    "           y=list(age_sex_fail.query('sex == \"M\"')['G3']), \n",
    "           text=list(age_sex_fail.query('sex == \"M\"')['G3']),\n",
    "           marker_color = ['red'] * 10,\n",
    "            showlegend=False\n",
    "\n",
    "          ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "fig.update_yaxes(title_text='Maschi', row=1, col=1, range=[0,200])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(name='Promossi', \n",
    "               x=list(age_sex_pass.query('sex == \"F\"')['age']), \n",
    "               y=list(age_sex_pass.query('sex == \"F\"')['G3']), \n",
    "               text=list(age_sex_pass.query('sex == \"F\"')['G3']),\n",
    "           marker_color = ['green'] * 10 ),\n",
    "    row=2,\n",
    "    col=1\n",
    ")\n",
    "fig.append_trace(\n",
    "    go.Bar(name='Bocciati', \n",
    "           x=list(age_sex_fail.query('sex == \"F\"')['age']), \n",
    "           y=list(age_sex_fail.query('sex == \"F\"')['G3']), \n",
    "           text=list(age_sex_fail.query('sex == \"F\"')['G3']) ,\n",
    "           marker_color = ['red'] * 10,\n",
    "    showlegend=True\n",
    "           \n",
    "          ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "fig.update_yaxes(title_text='Femmine', row=2, col=1, range=[0,200])\n",
    "\n",
    "\n",
    "# Change the bar mode\n",
    "fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text='Promossi/Bocciati per età e sesso',\n",
    "        y=0.90,\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        yanchor='top'\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='Età',\n",
    "        linecolor='rgb(204, 204, 204)',\n",
    "    ),\n",
    "    barmode='stack',\n",
    "    uniformtext_minsize=8,\n",
    "    uniformtext_mode='hide'    \n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age_sex_pass\n",
    "#age_sex_total['PROMOSSI'] = age_sex_pass[:-1]\n",
    "df.groupby(['age','sex']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing dei dati\n",
    "\n",
    "I dati contenuti nel DataFrame appena caricato non possono essere usati direttamente per l'addestramento di un modello di Machine Learning: la presenza di feature eterogenee (stringhe, interi e numeri floating point) deve essere gestita attraverso un'opportuna fase di preprocessing. Per poter addestrare un modello di Machine Learning è necessario convertire i dati del **DataFrame** in valori numerici e memorizzarli in un `ndarray`.\n",
    "Le features categoriche verranno quindi convertite in numeriche, utilizzando un'encoder (**OneHotEncoder**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tutte le features non numeriche, che vanno trasformate in features numeriche\n",
    "features_to_be_encoded= df_cat.columns\n",
    "print(f'features_to_be_encoded: {len(features_to_be_encoded)}')\n",
    "\n",
    "# Drop first encoded col\n",
    "one_hot_encoder = OneHotEncoder(drop='first',handle_unknown='error')\n",
    "#one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "encoded_features = one_hot_encoder.fit_transform(df[features_to_be_encoded])\n",
    "encoded_column_names = one_hot_encoder.get_feature_names(features_to_be_encoded)\n",
    "one_hot_encoded_frame =  pd.DataFrame(encoded_features.toarray(), columns=encoded_column_names)\n",
    "df_num_cat = df_num.join(one_hot_encoded_frame)\n",
    "df_encoded = df_num_cat.join(df_target)\n",
    "\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficienti di correlazione\n",
    "\n",
    "Verifichiamo se esistono eventuali correlazioni tra le diverse feature.\n",
    "Come ci aspettavamo `G1`,`G2`,`G3` hanno una forte correlazione tra di loro.\n",
    "Tra le altre features,`failures` (bocciature passate) e `Medu` (livello istruzione madre) potrebbero avere una certa correlazione con i voti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcolo dei coefficienti di correlazione tra le features\n",
    "corr_matrix = df_encoded.corr().abs()\n",
    "kot = corr_matrix[corr_matrix>=.15]\n",
    "plt.figure(figsize=(12,8))\n",
    "heatmap = sns.heatmap(kot, cmap=\"Greens\")\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize': 18}, pad=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "cut_off = 0.2  # only show cells with abs(correlation) at least this value\n",
    "extreme_1 = 0.75  # show with a star\n",
    "extreme_2 = 0.85  # show with a second star\n",
    "extreme_3 = 0.90  # show with a third star\n",
    "mask |= np.abs(corr) < cut_off\n",
    "corr = corr[~mask]  # fill in NaN in the non-desired cells\n",
    "\n",
    "remove_empty_rows_and_cols = True\n",
    "if remove_empty_rows_and_cols:\n",
    "    wanted_cols = np.flatnonzero(np.count_nonzero(~mask, axis=1))\n",
    "    wanted_rows = np.flatnonzero(np.count_nonzero(~mask, axis=0))\n",
    "    corr = corr.iloc[wanted_cols, wanted_rows]\n",
    "\n",
    "annot = [[f\"{val:.4f}\"\n",
    "          + ('' if abs(val) < extreme_1 else '\\n★')  # add one star if abs(val) >= extreme_1\n",
    "          + ('' if abs(val) < extreme_2 else '★')  # add an extra star if abs(val) >= extreme_2\n",
    "          + ('' if abs(val) < extreme_3 else '★')  # add yet an extra star if abs(val) >= extreme_3\n",
    "          for val in row] for row in corr.to_numpy()]\n",
    "heatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=annot, fmt='', cmap='BrBG')\n",
    "heatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize': 18}, pad=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tramite degli scatter plot verifichiamo se c'e' una relazione\n",
    "# tra la feature e il target\n",
    "def plot_scatter(dataframe, x_list, y):\n",
    "    num_cols = 4\n",
    "    num_rows = math.ceil(len(x_list)/num_cols)\n",
    "    plt.figure(figsize=(15,num_rows*4))\n",
    "\n",
    "    for i,x in enumerate(x_list, start=1):\n",
    "        plt.subplot(num_rows, num_cols, i)\n",
    "        plt.scatter(dataframe[x], dataframe[y])\n",
    "        plt.xlabel(x)\n",
    "        if i%num_cols==1:\n",
    "            plt.ylabel(y)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scelta delle feature\n",
    "Per scegliere le feature da usare plottiamo degli scatter plot.\n",
    "Analiziamo in ordine:\n",
    "- feature sui voti\n",
    "- feature numeriche\n",
    "- feature categoriche\n",
    "\n",
    "Nel primo caso e' abbastanza marcata una correlazione lineare tra `G1`,`G2` e `G3`.\\\n",
    "Nel secondo caso una leggerissima correlazione si potrebbe vedere nella feature `failures` e `age`.\\\n",
    "Nel terzo caso non si notano particolari correlazioni. L'unica feature da verificare e' `higher_yes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variabile dipendente\n",
    "y_features='G3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter sui voti (G1,G2,G3)\n",
    "plot_scatter(df_encoded, df_target.columns, 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter feature numeriche\n",
    "plot_scatter(df_encoded, df_num.columns, 'G3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter feature numeriche\n",
    "plot_scatter(df_encoded, encoded_column_names, 'G3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'analisi della correlation matrix tra `G3` e le feature conferma valori molto bassi di correlazione se si escludono `G1` e `G2`.\\\n",
    "Le uniche feature che superano **0.2** di correlazione sono:\n",
    "- `failures`\n",
    "- `higher_yes`\n",
    "- `Medu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# la feature che vogliamo predirre è G3 \n",
    "#print(len(df_num_cat.columns))\n",
    "\n",
    "corr_sorted = df_encoded.corr().abs()[y_features].sort_values(ascending=False)\n",
    "corr_sorted\n",
    "\n",
    "top_corr = corr_sorted[1:]\n",
    "print(f'top_correlation for {y_features} \\n{top_corr}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le feature che vogliamo usare per predirre\n",
    "#x_features=top_corr.index\n",
    "x_features=['failures','higher_yes','Medu']\n",
    "# usiamo Mother e Father education\n",
    "indexes = [df_encoded.columns.get_loc(col) for col in x_features]\n",
    "data_x = df_encoded.values[:,indexes]\n",
    "data_x = data_x.astype(np.float32)\n",
    "\n",
    "# data_y conterrà solo G3\n",
    "data_y = df_encoded[y_features].values\n",
    "data_y = data_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train e test\n",
    "Dividiamo il dataset in **train** (60%) e **test** (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_portion = 0.4\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=test_portion, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressione\n",
    "La libreria Scikit-learn mette a disposizione diversi algoritmi per la regressione. Durante questa esercitazione verrà fatto uso di LinearRegression.\n",
    "L'obiettivo è quello di minimizzare l'RMSE, ovvero lo scostamento tra il valore predetto e quello reale.\n",
    "Nella cella seguente è fornito il codice per creare e addestrare un'istanza di LinearRegression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento di un LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(train_x, train_y)\n",
    "\n",
    "# Ottenimento delle predizioni\n",
    "train_y_predicted = lin_reg.predict(train_x)\n",
    "\n",
    "# Calcolo del RMSE\n",
    "rmse = np.sqrt(mean_squared_error(train_y, train_y_predicted))\n",
    "print(f'Train RMSE: {rmse:.3f}') \n",
    "\n",
    "# Ottenimento delle predizioni (test) e calcolo RMSE\n",
    "test_y_predicted = lin_reg.predict(test_x)\n",
    "rmse = np.sqrt(mean_squared_error(test_y, test_y_predicted))\n",
    "print(f'Test RMSE: {rmse:.3f}') \n",
    "\n",
    "print(f'R2 score: {lin_reg.score(test_x, test_y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola l'errore come scostamento delle predizioni dal valore reale\n",
    "errors = np.abs(test_y - test_y_predicted) \n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Distribuzione degli errori\")\n",
    "plt.hist(x = errors, bins=12)\n",
    "plt.xlabel('Scostamento da G3')\n",
    "plt.ylabel('Osservazioni')\n",
    "plt.show()\n",
    "\n",
    "# Visualizza l'andamento reale e quello predetto\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Confronto tra valori reali e prediction\")\n",
    "plt.plot(test_y, label='Real')\n",
    "plt.plot(test_y_predicted, label='Prediction')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel('Osservazioni')\n",
    "plt.ylabel('G3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addestramento di un RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(train_x, train_y)\n",
    "\n",
    "# Ottenimento delle predizioni\n",
    "train_y_predicted = forest_reg.predict(train_x)\n",
    "\n",
    "# Calcolo del RMSE\n",
    "rmse = np.sqrt(mean_squared_error(train_y, train_y_predicted))\n",
    "print(f'Train RMSE: {rmse:.3f}') \n",
    "\n",
    "# Ottenimento delle predizioni (test) e calcolo RMSE\n",
    "test_y_predicted = forest_reg.predict(test_x)\n",
    "rmse = np.sqrt(mean_squared_error(test_y, test_y_predicted))\n",
    "print(f'Test RMSE: {rmse:.3f}') \n",
    "\n",
    "print(f'R2 score: {forest_reg.score(test_x, test_y):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcola l'errore come scostamento delle predizioni dal valore reale\n",
    "errors = np.abs(test_y - test_y_predicted) \n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Distribuzione degli errori\")\n",
    "plt.hist(x = errors, bins=12)\n",
    "plt.xlabel('Scostamento da G3')\n",
    "plt.ylabel('Osservazioni')\n",
    "plt.show()\n",
    "\n",
    "# Visualizza l'andamento reale e quello predetto\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.title(\"Confronto tra valori reali e prediction\")\n",
    "plt.plot(test_y, label='Real')\n",
    "plt.plot(test_y_predicted, label='Prediction')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.xlabel('Osservazioni')\n",
    "plt.ylabel('G3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso - feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_regression(alpha, predictors, data):\n",
    "    #Fit the model\n",
    "    reg = Lasso(alpha=alpha, normalize=True, max_iter=1e5)\n",
    "    reg.fit(data[predictors], data['G3'])\n",
    "    y_pred = reg.predict(data[predictors])\n",
    "        \n",
    "    # return the result in pre-defined format\n",
    "    rss = sum((y_pred - data['G3'])**2)\n",
    "    ret = []\n",
    "    ret.extend([reg.score(data[predictors],data['G3'])])\n",
    "    ret.extend([rss])\n",
    "    ret.extend([reg.intercept_])\n",
    "    ret.extend(reg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 2e-2, 1e-1, 1, 10]\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "predictors=df_encoded.select_dtypes(include=numerics).columns.drop(['G1', 'G2', 'G3'])\n",
    "\n",
    "cols = ['R2','rss','intercept']\n",
    "cols.extend(predictors)\n",
    "ind = [f'alpha_{alphas[i]}' for i in range(0, len(alphas))]\n",
    "coef_matrix = pd.DataFrame(index=ind, columns=cols)\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    coef_matrix.iloc[i,] = lasso_regression(alpha=alpha, predictors=predictors, data=df_encoded)\n",
    "    \n",
    "coef_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizzando Lasso abbiamo potuto constatare che purtroppo le feature ci dicono poco riguardo G3 (vanno tutte a 0 con alpha > 0.01)\n",
    "\n",
    "Le feature che si rivelano essere più forti (alpha = 0.01) sono: **[Medu, studytime, failures, Dalc, school_MS, address_U, schoolsup_yes, higher_yes]**\n",
    "\n",
    "Tuttavia **[studytime, Dalc, school_MS, address_U, schoolsup_yes]** sono molto vicine allo 0, quindi le scarteremo lo stesso\n",
    "\n",
    "Le nostre analisi fatte precedentemente con la correlazione confermano quindi che le feature migliori da utilizzare sono:\n",
    "\n",
    "**[Medu, failures, higher_yes]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la feature che vogliamo predirre\n",
    "y_features='G3'\n",
    "\n",
    "# le feature che vogliamo usare per predirre\n",
    "x_features=['Medu','failures','higher_yes']\n",
    "\n",
    "indexes = [df_encoded.columns.get_loc(col) for col in x_features]\n",
    "data_x = df_encoded.values[:,indexes]\n",
    "data_x = data_x.astype(np.float32)\n",
    "\n",
    "# data_y conterrà solo G3_category\n",
    "data_y = df_encoded[y_features].values\n",
    "data_y = data_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train e test\n",
    "test_portion = 0.4\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=test_portion, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "ridge = Ridge()\n",
    "\n",
    "alphas = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]\n",
    "parameters = {'alpha': alphas}\n",
    "\n",
    "grid_search_cv = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
    "\n",
    "grid_search_cv.fit(train_x, train_y)\n",
    "\n",
    "print(f'Best Params: {grid_search_cv.best_params_}')\n",
    "print(f'Best Score: {grid_search_cv.best_score_:.3f}')\n",
    "\n",
    "# Best estimator\n",
    "grid_search_cv = grid_search_cv.best_estimator_\n",
    "\n",
    "# Predizione\n",
    "train_y_predicted = grid_search_cv.predict(train_x)\n",
    "test_y_predicted = grid_search_cv.predict(test_x)\n",
    "\n",
    "# Calcolo del RMSE\n",
    "train_rmse = np.sqrt(mean_squared_error(train_y, train_y_predicted))\n",
    "test_rmse = np.sqrt(mean_squared_error(test_y, test_y_predicted))\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.3f}') \n",
    "print(f'Test RMSE: {test_rmse:.3f}') \n",
    "print(f'R2 score: {grid_search_cv.score(test_x, test_y):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusione sul regressore\n",
    "Il dataset contiene troppe feature categoriche di conseguenza il regressore non riesce ad essere abbastanza preciso.\\\n",
    "Come ci aspettavamo le uniche feature usabili in questo dataset per fare una buona regressione su `G3` sono `G1` e `G2`.\\\n",
    "Usando le altre feature l'R2 score e' molto basso (Linear=0.142, RandomForest=0.136, Ridge=0.140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggiunta colonna artificiale per la classificazione\n",
    "Proviamo ora a classificare gli alunni in **bocciati** e **promossi**.\\\n",
    "Introduciamo una nuova variabile dipendente `G3_category` mappando a `0` i valori di `G3<10` e a `1` quelli superiori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = bocciato\n",
    "# 1 = promosso\n",
    "for score in df['G3']:\n",
    "    if score < 10:\n",
    "        g3_category = 0\n",
    "    else:\n",
    "        g3_category = 1\n",
    "    df.loc[df['G3']==score, 'G3_category'] = g3_category\n",
    "    df_encoded.loc[df['G3']==score, 'G3_category'] = g3_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappresentiamo graficamente i promossi e i bocciati in relazione alle features `failures` e `Medu`.\\\n",
    "Purtroppo non e' possibile evidenziare un pattern particolare. Proveremo quindi a usare i classificatori conosciuti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "X = df_encoded['Medu']\n",
    "Y = df_encoded['failures']\n",
    "C = df_encoded['G3_category']\n",
    "X1,Y1,_ = zip(*[(x,y,c) for x,y,c in zip(X,Y,C) if c==1])\n",
    "ax.scatter(X1, Y1, c='g', marker=mrk.CARETUPBASE, s=100, label='promossi')   \n",
    "\n",
    "X2,Y2,_ = zip(*[(x,y,c) for x,y,c in zip(X,Y,C) if c==0])\n",
    "ax.scatter(X2, Y2, c='r', marker=mrk.CARETDOWNBASE, s=100, label='bocciati')\n",
    "plt.xlabel('Medu')\n",
    "plt.ylabel('failures')\n",
    "plt.legend(bbox_to_anchor=(1, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train e test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la feature che vogliamo predirre\n",
    "y_features='G3_category'\n",
    "\n",
    "# le feature che vogliamo usare per predirre\n",
    "#x_features=['failures','higher_yes','Medu', 'studytime','Fedu','Dalc','school_MS','age','reason_reputation','address_U','Walc','internet_yes']\n",
    "x_features=['failures','higher_yes','Medu']\n",
    "\n",
    "indexes = [df_encoded.columns.get_loc(col) for col in x_features]\n",
    "data_x = df_encoded.values[:,indexes]\n",
    "data_x = data_x.astype(np.float32)\n",
    "\n",
    "# data_y conterrà solo G3_category\n",
    "data_y = df_encoded[y_features].values\n",
    "data_y = data_y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train e test\n",
    "test_portion = 0.4\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=test_portion, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizzazione della scala\n",
    "Per dare a tutte le feature lo stesso peso, standardizziamo i valori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizzo tutti i valori nel range [0, 1.0]\n",
    "#train_x = preprocessing.normalize(train_x, norm='l2')\n",
    "#test_x = preprocessing.normalize(test_x, norm='l2')\n",
    "#train_x\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "test_x = scaler.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definizione funzioni utili per valutazione classificatore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display confusion matrix\n",
    "def plot_confusion_matrix(test_y, prediction):\n",
    "    cmat = confusion_matrix(test_y, prediction)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(pd.DataFrame(cmat), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    print(f'TP - True Negative: {cmat[0,0]}')\n",
    "    print(f'FP - False Positive: {cmat[0,1]}')\n",
    "    print(f'FN - False Negative: {cmat[1,0]}')\n",
    "    print(f'TP - True Positive: {cmat[1,1]}')\n",
    "    print(f'Accuracy Rate: {np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)):.3f}')\n",
    "    print(f'Misclassification Rate: {np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat)):.3f}')\n",
    "\n",
    "# display ROC curve\n",
    "def plot_roc_curve(test_y, prediction):\n",
    "    fpr, tpr, _ = metrics.roc_curve(test_y,  prediction)\n",
    "    auc = metrics.roc_auc_score(test_y, prediction)\n",
    "\n",
    "    fig = px.area(\n",
    "        x=fpr, y=tpr,\n",
    "        title=f'ROC Curve (AUC={auc:.4f})',\n",
    "        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n",
    "        width=700, height=500\n",
    "    )\n",
    "    fig.add_shape(\n",
    "        type='line', line=dict(dash='dash'),\n",
    "        x0=0, x1=1, y0=0, y1=1\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n",
    "    fig.update_xaxes(constrain='domain')\n",
    "    fig.show()\n",
    "    \n",
    "# plot error rate\n",
    "def plot_error_rate(errors):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, 40), errors, color='red', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('Error Rate K Value')\n",
    "    plt.xlabel('K Value')\n",
    "    plt.ylabel('Mean Error')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione Dummy\n",
    "Partiamo da un DummyClassfier per poi verificare se riusciamo a far meglio con KNN e SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "clf.fit(train_x, train_y)\n",
    "prediction = clf.predict(test_x)\n",
    "# display confusion matrix\n",
    "plot_confusion_matrix(test_y, prediction)\n",
    "# display ROC curve for best prediction\n",
    "plot_roc_curve(test_y, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusione sul classificatore Dummy\n",
    "L'AUC del DummyClassifier e' 0.5 che equivale a una classificazione random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificazione con KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo errori per diverse istanze di K Neighbors Neighbors Classifier\n",
    "Applichiamo **KNN** con diversi valori di `K` per trovare il miglior K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "predictions = []\n",
    "\n",
    "# Calculating error for K values between 1 and 40\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(train_x, train_y)\n",
    "    predictions.append(knn.predict(test_x))\n",
    "    errors.append(np.mean(predictions[i-1] != test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot error rate\n",
    "plot_error_rate(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizza la miglior predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_best = errors.index(min(errors))\n",
    "best_prediction = predictions[idx_best]\n",
    "print(f\"Best prediction with K={idx_best + 1}\")\n",
    "\n",
    "# display confusion matrix for best prediction\n",
    "plot_confusion_matrix(test_y, best_prediction)\n",
    "# display ROC curve for best prediction\n",
    "plot_roc_curve(test_y, best_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out classification report and confusion matrix\n",
    "print(classification_report(test_y, best_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusione sul classificatore KNN\n",
    "La ROC curve mostra come il classificatore KNN (in questo caso) non dia buoni risultati, infatti l'AUC e' leggermente piu' alto 0.614 rispetto a quanto fatto dal DummyClassifier 0.5 (poco piu' di una classificazione random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search e Classificazione con SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "param_grid = [{\n",
    "    'C': [1, 10, 100, 200, 300], \n",
    "    'gamma': [100, 10, 1, 0.1, 0.01, 0.001, 0.0001],\n",
    "    'kernel': ['rbf', 'linear'], \n",
    "    # ATTENZIONE: poly e' molto lento, definisco un max iter\n",
    "    #'kernel': ['poly'], 'max_iter': [500000], 'degree': [3],\n",
    "    'class_weight': ['balanced']\n",
    "}]\n",
    "\n",
    "# Numero di fold per la Cross-validation\n",
    "n_folds = 3\n",
    "# Creazione di un oggetto di tipo GridSearchCV\n",
    "grid_search_cv = GridSearchCV(clf, param_grid, cv=n_folds)\n",
    "# Esecuzione della ricerca degli iperparametri \n",
    "grid_search_cv.fit(train_x, train_y)\n",
    "\n",
    "best_clf = SVC(**grid_search_cv.best_params_)\n",
    "best_clf.fit(train_x, train_y)\n",
    "best_prediction = best_clf.predict(test_x)\n",
    "\n",
    "print(f'Migior parametri: {grid_search_cv.best_params_}')\n",
    "print(f'Miglior Accurancy: {grid_search_cv.best_score_:.3f}')\n",
    "# display confusion matrix for best prediction\n",
    "plot_confusion_matrix(test_y, best_prediction)\n",
    "\n",
    "# display ROC curve for best prediction\n",
    "plot_roc_curve(test_y, best_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusione sul classificatore SVM\n",
    "Abbiamo applicato SVC con diversi parametri. Usando Grid Search abbiamo trovato il migliore.  \n",
    "Anche in questo caso l'AUC e' un po' meglio del DummyClassfier ma il suo valore non e' altissimo 0.628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusione\n",
    "I risultati di questi test hanno evidenziato come, escludendo `G1` e `G2` e usando solo dati socio-demografici, non sia possibile predire se uno studente sara' promosso o meno. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
